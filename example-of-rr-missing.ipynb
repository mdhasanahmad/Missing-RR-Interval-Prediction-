{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14518958,"sourceType":"datasetVersion","datasetId":9272984}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-20T06:04:43.501214Z","iopub.execute_input":"2026-01-20T06:04:43.501566Z","iopub.status.idle":"2026-01-20T06:04:43.505283Z","shell.execute_reply.started":"2026-01-20T06:04:43.501539Z","shell.execute_reply":"2026-01-20T06:04:43.504624Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install wfdb neurokit2 numpy scipy scikit-learn xgboost torch tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T06:04:43.506724Z","iopub.execute_input":"2026-01-20T06:04:43.507025Z","iopub.status.idle":"2026-01-20T06:04:55.400793Z","shell.execute_reply.started":"2026-01-20T06:04:43.506991Z","shell.execute_reply":"2026-01-20T06:04:55.400018Z"}},"outputs":[{"name":"stdout","text":"Collecting wfdb\n  Downloading wfdb-4.3.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting neurokit2\n  Downloading neurokit2-0.2.12-py2.py3-none-any.whl.metadata (37 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.15.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\nRequirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\nRequirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.3)\nRequirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.10.0)\nRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.10.0)\nCollecting pandas>=2.2.3 (from wfdb)\n  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.5)\nRequirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\nRequirement already satisfied: PyWavelets>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from neurokit2) (1.9.0)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (4.60.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (26.0rc2)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (11.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.5)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2026.1.4)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.23)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\nDownloading wfdb-4.3.0-py3-none-any.whl (163 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading neurokit2-0.2.12-py2.py3-none-any.whl (708 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m708.4/708.4 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: pandas, wfdb, neurokit2\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.2\n    Uninstalling pandas-2.2.2:\n      Successfully uninstalled pandas-2.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ndask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ncudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed neurokit2-0.2.12 pandas-2.3.3 wfdb-4.3.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================\n# Kaggle Notebook (FULL SOLVED CODE - Robust WFDB Loader)\n# RR Imputation GAN (LSTM residual + WGAN-GP) with:\n# ✅ Robust WFDB RR loader (auto-detect annotation extension)\n# ✅ Per-record normalization (fixes trending outliers)\n# ✅ Missing-only L1 + dRR consistency + WGAN-GP\n# ============================================================\n\nimport os, random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nimport wfdb\n\n# -------------------------\n# CONFIG\n# -------------------------\nDATASET_ROOT = \"/kaggle/input/normal-sinus-dataset/normal-sinus-rhythm-rr-interval-database-1.0.0\"\n\nWINDOW_LEN   = 50\nSTRIDE       = 5\nK_MISSING    = 5\n\nBATCH_SIZE   = 128\nEPOCHS       = 30\nN_CRITIC     = 3\n\nLR_G         = 2e-4\nLR_D         = 2e-4\n\nLAMBDA_MISS   = 200.0\nLAMBDA_DRR    = 5.0\nLAMBDA_SMOOTH = 0.02\n\nGP_LAMBDA     = 10.0\nFILL_MODE     = \"mean\"   # \"mean\" or \"ffill\"\nSEED          = 42\n\n# -------------------------\n# Seed\n# -------------------------\ndef seed_all(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nseed_all(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# ============================================================\n# 1) Robust RR loader (auto-detect annotation extension)\n# ============================================================\ndef list_record_names(dataset_root):\n    hea_files = [f for f in os.listdir(dataset_root) if f.endswith(\".hea\")]\n    return sorted(set(os.path.splitext(f)[0] for f in hea_files))\n\ndef find_ann_ext_for_record(dataset_root, rec):\n    \"\"\"\n    Find possible annotation extensions for a record by scanning files:\n      rec.<ext> where ext not in {hea, dat}\n    Return list of candidate extensions.\n    \"\"\"\n    candidates = []\n    prefix = rec + \".\"\n    for fn in os.listdir(dataset_root):\n        if fn.startswith(prefix):\n            ext = fn.split(\".\")[-1]\n            if ext.lower() not in [\"hea\", \"dat\"]:\n                candidates.append(ext)\n    # common ones first if present\n    priority = [\"atr\", \"ecg\", \"qrs\", \"ann\"]\n    candidates_sorted = []\n    for p in priority:\n        if p in candidates:\n            candidates_sorted.append(p)\n    for c in candidates:\n        if c not in candidates_sorted:\n            candidates_sorted.append(c)\n    return candidates_sorted\n\ndef rr_from_annotation(dataset_root, rec, ext):\n    rec_path = os.path.join(dataset_root, rec)\n    header = wfdb.rdheader(rec_path)\n    fs = float(header.fs)\n\n    ann = wfdb.rdann(rec_path, ext)\n    r_samples = ann.sample.astype(np.float32)\n\n    # RR in ms\n    rr_ms = np.diff(r_samples) / fs * 1000.0\n\n    # physiological filter\n    rr_ms = rr_ms[(rr_ms > 300) & (rr_ms < 2000)]\n    return rr_ms.astype(np.float32)\n\ndef load_rr_records(dataset_root):\n    record_names = list_record_names(dataset_root)\n    if len(record_names) == 0:\n        raise RuntimeError(\"No .hea files found. Check DATASET_ROOT.\")\n\n    print(\"Found .hea records:\", len(record_names))\n\n    rr_records = []\n    loaded = 0\n    failed = 0\n\n    for rec in record_names:\n        exts = find_ann_ext_for_record(dataset_root, rec)\n        got = None\n        for ext in exts:\n            try:\n                rr = rr_from_annotation(dataset_root, rec, ext)\n                if len(rr) >= 60:\n                    got = rr\n                    break\n            except:\n                continue\n\n        if got is None:\n            failed += 1\n        else:\n            rr_records.append(got)\n            loaded += 1\n\n    if loaded == 0:\n        # helpful debug: show one record's available files\n        sample = record_names[0]\n        sample_files = [f for f in os.listdir(dataset_root) if f.startswith(sample + \".\")]\n        raise RuntimeError(\n            \"No RR records loaded. WFDB couldn't read any annotation.\\n\"\n            f\"Example record '{sample}' files: {sample_files}\\n\"\n            \"Your dataset might store peaks differently.\"\n        )\n\n    print(f\"Loaded RR records: {loaded} | failed: {failed}\")\n    return rr_records\n\nrr_records = load_rr_records(DATASET_ROOT)\nprint(\"RR lengths (min/mean/max):\",\n      min(len(r) for r in rr_records),\n      int(np.mean([len(r) for r in rr_records])),\n      max(len(r) for r in rr_records))\n\n# ============================================================\n# 2) Dataset (per-record normalization + windowing)\n# ============================================================\nclass RRWindowDataset(Dataset):\n    def __init__(self, rr_records, window_len=50, stride=5):\n        self.samples = []\n        self.stats = []  # (mu, std) per window\n\n        for rec in rr_records:\n            rec = np.asarray(rec, dtype=np.float32)\n            if len(rec) < window_len:\n                continue\n\n            mu = float(rec.mean())\n            std = float(rec.std() + 1e-8)\n            rec_norm = (rec - mu) / std\n\n            for s in range(0, len(rec_norm) - window_len + 1, stride):\n                self.samples.append(rec_norm[s:s+window_len].astype(np.float32))\n                self.stats.append((mu, std))\n\n        if len(self.samples) == 0:\n            raise ValueError(\"No windows created. Reduce WINDOW_LEN/STRIDE or check lengths.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        x = torch.tensor(self.samples[idx], dtype=torch.float32)\n        mu, std = self.stats[idx]\n        return x, torch.tensor(mu, dtype=torch.float32), torch.tensor(std, dtype=torch.float32)\n\n# ============================================================\n# 3) Masking\n# ============================================================\ndef make_mask_and_corrupt(x, K=5, fill_mode=\"mean\"):\n    B, T = x.shape\n    m = torch.ones((B, T), device=x.device)\n    idx_missing_list = []\n\n    for b in range(B):\n        idx = torch.randperm(T, device=x.device)[:K]\n        m[b, idx] = 0.0\n        idx_missing_list.append(idx)\n\n    if fill_mode == \"mean\":\n        obs_sum = (x * m).sum(dim=1, keepdim=True)\n        obs_cnt = m.sum(dim=1, keepdim=True).clamp(min=1.0)\n        fill = obs_sum / obs_cnt\n        x_obs = x * m + fill * (1.0 - m)\n    elif fill_mode == \"ffill\":\n        x_obs = x.clone()\n        for b in range(B):\n            missing = (m[b] == 0)\n            obs_idx = torch.where(m[b] == 1)[0]\n            if len(obs_idx) == 0:\n                continue\n            prev = x_obs[b, obs_idx[0]].item()\n            for t in range(T):\n                if missing[t]:\n                    x_obs[b, t] = prev\n                else:\n                    prev = x_obs[b, t].item()\n    else:\n        raise ValueError(\"fill_mode must be 'mean' or 'ffill'\")\n\n    return x_obs, m, idx_missing_list\n\n# ============================================================\n# 4) Models\n# ============================================================\nclass LSTMGeneratorResidual(nn.Module):\n    def __init__(self, hidden=64, num_layers=2, dropout=0.1):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size=4,\n            hidden_size=hidden,\n            num_layers=num_layers,\n            dropout=dropout if num_layers > 1 else 0.0,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.head = nn.Sequential(\n            nn.Linear(hidden * 2, hidden),\n            nn.ReLU(),\n            nn.Linear(hidden, 1)\n        )\n\n    def forward(self, x_base, m, noise):\n        dx = x_base[:, 1:] - x_base[:, :-1]\n        dx = torch.cat([torch.zeros((x_base.size(0), 1), device=x_base.device), dx], dim=1)\n        inp = torch.stack([x_base, m, dx, noise], dim=-1)\n        h, _ = self.lstm(inp)\n        delta = self.head(h).squeeze(-1)\n        return delta\n\nclass CNNWganCritic(nn.Module):\n    def __init__(self, channels=64, kernel=5):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv1d(1, channels, kernel, padding=kernel//2),\n            nn.LeakyReLU(0.2),\n            nn.Conv1d(channels, channels, kernel, padding=kernel//2),\n            nn.LeakyReLU(0.2),\n            nn.Conv1d(channels, channels, kernel, padding=kernel//2),\n            nn.LeakyReLU(0.2),\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Linear(channels, 1)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        h = self.net(x)\n        h = self.pool(h).squeeze(-1)\n        return self.fc(h).squeeze(-1)\n\n# ============================================================\n# 5) WGAN-GP\n# ============================================================\ndef gradient_penalty(critic, real, fake, gp_lambda=10.0):\n    B = real.size(0)\n    eps = torch.rand(B, 1, device=real.device).expand_as(real)\n    x_hat = eps * real + (1 - eps) * fake\n    x_hat.requires_grad_(True)\n\n    scores = critic(x_hat)\n    grads = torch.autograd.grad(\n        outputs=scores,\n        inputs=x_hat,\n        grad_outputs=torch.ones_like(scores),\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True\n    )[0]\n\n    grads = grads.view(B, -1)\n    gp = ((grads.norm(2, dim=1) - 1.0) ** 2).mean() * gp_lambda\n    return gp\n\n# ============================================================\n# 6) Train\n# ============================================================\ndef train_wgan(rr_records):\n    ds = RRWindowDataset(rr_records, window_len=WINDOW_LEN, stride=STRIDE)\n    dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n    print(\"Total windows:\", len(ds))\n\n    G = LSTMGeneratorResidual(hidden=64, num_layers=2, dropout=0.1).to(device)\n    D = CNNWganCritic(channels=64, kernel=5).to(device)\n\n    opt_g = optim.Adam(G.parameters(), lr=LR_G, betas=(0.5, 0.9))\n    opt_d = optim.Adam(D.parameters(), lr=LR_D, betas=(0.5, 0.9))\n\n    lo, hi = -6.0, 6.0  # normalized clamp\n\n    for ep in range(1, EPOCHS + 1):\n        G.train(); D.train()\n        d_sum = g_sum = miss_sum = drr_sum = 0.0\n\n        for x, mu, std in dl:\n            x = x.to(device)\n\n            # ----- Critic -----\n            for _ in range(N_CRITIC):\n                with torch.no_grad():\n                    x_base, m, _ = make_mask_and_corrupt(x, K=K_MISSING, fill_mode=FILL_MODE)\n                    noise = torch.rand_like(x) * (1.0 - m)\n                    delta = G(x_base, m, noise)\n                    x_hat = m * x + (1.0 - m) * (x_base + delta)\n                    x_hat = torch.clamp(x_hat, lo, hi)\n\n                real_score = D(x)\n                fake_score = D(x_hat)\n                gp = gradient_penalty(D, x, x_hat, gp_lambda=GP_LAMBDA)\n                d_loss = (fake_score.mean() - real_score.mean()) + gp\n\n                opt_d.zero_grad(set_to_none=True)\n                d_loss.backward()\n                opt_d.step()\n\n            # ----- Generator -----\n            x_base, m, _ = make_mask_and_corrupt(x, K=K_MISSING, fill_mode=FILL_MODE)\n            noise = torch.rand_like(x) * (1.0 - m)\n            delta = G(x_base, m, noise)\n            x_hat = m * x + (1.0 - m) * (x_base + delta)\n            x_hat = torch.clamp(x_hat, lo, hi)\n\n            adv = -D(x_hat).mean()\n\n            denom = (1.0 - m).sum().clamp(min=1.0)\n            miss_mae = (torch.abs(x_hat - x) * (1.0 - m)).sum() / denom\n\n            m_pair = m[:, 1:] * m[:, :-1]\n            miss_drr_mask = 1.0 - m_pair\n            drr_hat = x_hat[:, 1:] - x_hat[:, :-1]\n            drr_true = x[:, 1:] - x[:, :-1]\n            denom_drr = miss_drr_mask.sum().clamp(min=1.0)\n            drr_loss = (torch.abs(drr_hat - drr_true) * miss_drr_mask).sum() / denom_drr\n\n            smooth = torch.mean(torch.abs(drr_hat))\n\n            g_loss = adv + LAMBDA_MISS * miss_mae + LAMBDA_DRR * drr_loss + LAMBDA_SMOOTH * smooth\n\n            opt_g.zero_grad(set_to_none=True)\n            g_loss.backward()\n            opt_g.step()\n\n            d_sum += d_loss.item()\n            g_sum += g_loss.item()\n            miss_sum += miss_mae.item()\n            drr_sum += drr_loss.item()\n\n        print(\n            f\"Epoch {ep:03d} | D={d_sum/len(dl):.4f} | G={g_sum/len(dl):.4f} | \"\n            f\"MissMAE={miss_sum/len(dl):.4f} | dRRloss={drr_sum/len(dl):.4f}\"\n        )\n\n    return G, D\n\n# ============================================================\n# 7) Inference + Plot (de-normalize)\n# ============================================================\n@torch.no_grad()\ndef impute_one_window_from_record(G, rr_record, start_idx=0, K=5):\n    rr_record = np.asarray(rr_record, dtype=np.float32)\n    window = rr_record[start_idx:start_idx+WINDOW_LEN]\n    if len(window) < WINDOW_LEN:\n        raise ValueError(\"Not enough length for this window.\")\n\n    mu = float(window.mean())\n    std = float(window.std() + 1e-8)\n    x = (window - mu) / std\n\n    x = torch.tensor(x, dtype=torch.float32, device=device).unsqueeze(0)\n    x_base, m, idx_list = make_mask_and_corrupt(x, K=K, fill_mode=FILL_MODE)\n    idx_missing = idx_list[0].detach().cpu().numpy()\n\n    noise = torch.rand_like(x) * (1.0 - m)\n    delta = G(x_base, m, noise)\n    x_hat = m * x + (1.0 - m) * (x_base + delta)\n\n    x_true = x.squeeze(0).cpu().numpy() * std + mu\n    x_base_np = x_base.squeeze(0).cpu().numpy() * std + mu\n    x_hat_np  = x_hat.squeeze(0).cpu().numpy() * std + mu\n\n    err = x_hat_np[idx_missing] - x_true[idx_missing]\n    mae = float(np.mean(np.abs(err)))\n    rmse = float(np.sqrt(np.mean(err**2)))\n    return x_true, x_base_np, x_hat_np, idx_missing, mae, rmse\n\ndef plot_imputation(x_true, x_base, x_imp, idx_missing, title):\n    plt.figure(figsize=(12,4))\n    plt.plot(x_true, label=\"True RR\")\n    plt.plot(x_base, label=\"Masked/Filled RR\")\n    plt.plot(x_imp, label=\"Imputed RR\")\n    plt.scatter(idx_missing, x_true[idx_missing], marker=\"x\", s=80, label=\"Missing true\")\n    plt.scatter(idx_missing, x_imp[idx_missing], marker=\"o\", s=80, label=\"Missing pred\")\n    plt.title(title)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n# ============================================================\n# 8) Run\n# ============================================================\nG, D = train_wgan(rr_records)\n\n# Test plots on outlier-like records (by index in rr_records list)\nfor rid in [1, 39]:\n    x_true, x_base, x_imp, idx_m, mae, rmse = impute_one_window_from_record(G, rr_records[rid], start_idx=0, K=K_MISSING)\n    plot_imputation(x_true, x_base, x_imp, idx_m, title=f\"Record {rid} | MAE={mae:.4f} RMSE={rmse:.4f}\")\n\n# Save generator\nMODEL_PATH = \"/kaggle/working/rr_imputer_lstm_wgangp_residual_autoload.pt\"\ntorch.save({\"G_state\": G.state_dict(), \"window_len\": WINDOW_LEN, \"k_missing\": K_MISSING}, MODEL_PATH)\nprint(\"Saved:\", MODEL_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T06:07:54.051687Z","iopub.execute_input":"2026-01-20T06:07:54.052531Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nFound .hea records: 54\nLoaded RR records: 54 | failed: 0\nRR lengths (min/mean/max): 76760 107178 136481\nTotal windows: 1157023\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:829: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:179.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","output_type":"stream"},{"name":"stdout","text":"Epoch 001 | D=-0.0303 | G=31.6638 | MissMAE=0.1229 | dRRloss=0.1218\nEpoch 002 | D=-0.0439 | G=35.5945 | MissMAE=0.1093 | dRRloss=0.1089\nEpoch 003 | D=-0.0433 | G=35.9308 | MissMAE=0.1062 | dRRloss=0.1059\nEpoch 004 | D=-0.0439 | G=33.3240 | MissMAE=0.1044 | dRRloss=0.1042\nEpoch 005 | D=-0.0432 | G=31.3432 | MissMAE=0.1033 | dRRloss=0.1031\nEpoch 006 | D=-0.0424 | G=30.6946 | MissMAE=0.1029 | dRRloss=0.1027\nEpoch 007 | D=-0.0425 | G=29.5446 | MissMAE=0.1023 | dRRloss=0.1021\nEpoch 008 | D=-0.0426 | G=28.8314 | MissMAE=0.1019 | dRRloss=0.1017\nEpoch 009 | D=-0.0423 | G=26.4312 | MissMAE=0.1016 | dRRloss=0.1014\nEpoch 010 | D=-0.0425 | G=27.1099 | MissMAE=0.1010 | dRRloss=0.1009\nEpoch 011 | D=-0.0422 | G=25.9321 | MissMAE=0.1009 | dRRloss=0.1008\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}